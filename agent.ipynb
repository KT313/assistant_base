{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36a36a63-ee59-43df-aac7-20bacd32dd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobi/miniconda3/envs/assba/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/tobi/miniconda3/envs/assba/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import requests\n",
    "import base64\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "import gc\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM, BitsAndBytesConfig, AutoProcessor, PaliGemmaForConditionalGeneration, GPTQConfig\n",
    "from exllamav2 import ExLlamaV2, ExLlamaV2Config, ExLlamaV2Cache, ExLlamaV2Tokenizer\n",
    "from exllamav2.generator import ExLlamaV2Sampler, ExLlamaV2DynamicGenerator, ExLlamaV2BaseGenerator\n",
    "from flask import Flask, render_template, request, jsonify\n",
    "from scipy.special import softmax\n",
    "from typing import Union, List\n",
    "from llama_cpp import Llama\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import bitsandbytes\n",
    "import numpy as np\n",
    "import flash_attn\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "61c7bdc8-8ae3-42c8-a9a1-f0de4570e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_llm(task_input, system_instruct=None, forced_beginning=None, directly_get_reply=True):\n",
    "    inputs = {}\n",
    "    \n",
    "    inputs['chat'] = []\n",
    "    if system_instruct != None and system_instruct.strip() != \"\":\n",
    "        inputs['chat'].append({\"role\": \"system\", \"content\": system_instruct})\n",
    "    inputs['chat'].append({\"role\": \"User\", \"content\": task_input})\n",
    "    if forced_beginning != None and forced_beginning.strip() != \"\":\n",
    "        inputs['chat'].append({\"role\": \"AI\", \"content\": forced_beginning})\n",
    "        \n",
    "    inputs['model'] = \"gemma-2-9b\"\n",
    "    inputs['model_dtype'] = \"bfloat16\"\n",
    "    inputs['max_new_tokens'] = \"512\"\n",
    "    inputs['debugmode'] = True\n",
    "\n",
    "    \n",
    "    url = f\"http://127.0.0.1:10000/infer\"\n",
    "    response = requests.post(url, json=inputs) \n",
    "    if response.json()['status'] == \"error\":\n",
    "        print(response.json()['error-info'])\n",
    "        return json.dumps(response.json())\n",
    "    ai_reply = response.json()['returned_content'][0]\n",
    "    info = response.json()['info']\n",
    "    \n",
    "    returned_content = [\n",
    "        {'role': 'AI', 'content': \"\\n\".join(ai_reply)}\n",
    "    ]\n",
    "    if directly_get_reply:\n",
    "        return returned_content[-1]['content']\n",
    "    else:\n",
    "        return json.dumps({'status': 'success', 'returned_content': returned_content, 'info': info})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d1d97278-5742-490a-82d8-7f772f80ade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"Rewrite the following specific task into a general, symbolized task description.\n",
    "Use placeholders for names, numbers, and items.\n",
    "Example:\n",
    "User:\n",
    "Peter has 5 apples and 7 bananas. If he eats 3 apples and trades one apple for a banana, what is he left with?\n",
    "\n",
    "Answer:\n",
    "{\n",
    "    \"symbolized task\": \"[main object] has [number 1] [item 1] and [number 2] [item 2].\n",
    "[number 3] of [item 1] are removed.\n",
    "[number 4] of [item 1] is changed into [number 5] of [item 2].\n",
    "What are the values of [item 1] and [item 2]?\"\n",
    "}\n",
    "\n",
    "User:\n",
    "Peter has 5 candles that are all the same length. He lights them all at the same time. After a while, he blows out the candles one after the other. Which of the five candles was the first one he has blown out?\n",
    "Here is a figure of the five candles after they have been blown out. The number of = represents the length of the candle. Respond with the label of the candle that has been blown out first by Peter.\n",
    "1) ====\n",
    "2) =======\n",
    "3) ========\n",
    "4) =\n",
    "5) ==\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "70867465-5a92-4ddd-8e6a-1f0f7c6dc544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Peter has [number] [item] that are all the same [attribute]. \n",
      "    Peter lights them all at the same time. \n",
      "    After a while, he blows out one [item] after the other. \n",
      "    Which [item] was the first one he blew out?\n",
      "    Here is a figure of the five [item] after they have been blown out. \n",
      "    The number of [item] represents the length of the [item]. \n",
      "    Respond with the label of the [item] that was first blown out by Peter.\"\n",
      "} \n",
      "\n",
      "\n",
      "\n",
      "Let me know if you'd like to try another one!\n"
     ]
    }
   ],
   "source": [
    "print(use_llm(\n",
    "    task_input = query,\n",
    "    forced_beginning = \"{\\n    \\\"symbolized task\\\": \"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf3e77c6-409f-4ddb-aa79-607619ed0bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(use_llm(\n",
    "    task_description = \"You will be given a string. decide if it is an english noun / pronoun (\\\"task_solution\\\": true) or not (\\\"task_solution\\\": false). reply in json format.\", \n",
    "    task_input = \"\\\"narr\\\"\",\n",
    "    forced_beginning = \"{\\n    \\\"task_solution\\\": \"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a562989-5da4-4ec1-97f5-bf5f358aa25a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
