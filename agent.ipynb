{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36a36a63-ee59-43df-aac7-20bacd32dd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobi/miniconda3/envs/assba/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/tobi/miniconda3/envs/assba/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import requests\n",
    "import base64\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "import gc\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM, BitsAndBytesConfig, AutoProcessor, PaliGemmaForConditionalGeneration, GPTQConfig\n",
    "from exllamav2 import ExLlamaV2, ExLlamaV2Config, ExLlamaV2Cache, ExLlamaV2Tokenizer\n",
    "from exllamav2.generator import ExLlamaV2Sampler, ExLlamaV2DynamicGenerator, ExLlamaV2BaseGenerator\n",
    "from flask import Flask, render_template, request, jsonify\n",
    "from scipy.special import softmax\n",
    "from typing import Union, List\n",
    "from llama_cpp import Llama\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import bitsandbytes\n",
    "import numpy as np\n",
    "import flash_attn\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61c7bdc8-8ae3-42c8-a9a1-f0de4570e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_llm(task_description, task_input, forced_beginning=None, directly_get_reply=True):\n",
    "    inputs = {}\n",
    "    inputs['chat'] = [\n",
    "        {\"role\": \"system\", \"content\": task_description},\n",
    "        {\"role\": \"User\", \"content\": task_input},\n",
    "    ]\n",
    "    if forced_beginning != None and forced_beginning.strip() != \"\":\n",
    "        inputs['chat'].append({\"role\": \"AI\", \"content\": forced_beginning})\n",
    "    inputs['model'] = \"gemma-2-9b\"\n",
    "    inputs['model_dtype'] = \"bfloat16\"\n",
    "    inputs['max_new_tokens'] = \"512\"\n",
    "    inputs['debugmode'] = True\n",
    "    inputs['images'] = []\n",
    "    inputs['agent_task_mode'] = False\n",
    "    inputs['use_functions'] = False\n",
    "    inputs['use_voiceinput'] = False\n",
    "    inputs['use_voiceoutput'] = False\n",
    "    inputs['allow_imagegen'] = False\n",
    "    inputs['beam_config'] = {\n",
    "        \"use_beam_search\": False,\n",
    "        \"max_num_beams\": \"1\",\n",
    "        \"depth_beams\": \"1\",\n",
    "        \"min_conf_for_sure\": \"0.95\",\n",
    "        \"min_conf_for_consider\": \"0.02\",\n",
    "        \"prob_sum_for_search\": \"0.98\",\n",
    "    }\n",
    "\n",
    "    \n",
    "    url = f\"http://127.0.0.1:10000/infer\"\n",
    "    response = requests.post(url, json=inputs) \n",
    "    if response.json()['status'] == \"error\":\n",
    "        print(response.json()['error-info'])\n",
    "        return json.dumps(response.json())\n",
    "    ai_reply = response.json()['returned_content'][0]\n",
    "    info = response.json()['info']\n",
    "    \n",
    "    returned_content = [\n",
    "        {'role': 'AI', 'content': \"\\n\".join(ai_reply)}\n",
    "    ]\n",
    "    if directly_get_reply:\n",
    "        return returned_content[-1]['content']\n",
    "    else:\n",
    "        return json.dumps({'status': 'success', 'returned_content': returned_content, 'info': info})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d355b921-efa7-46f0-9a4c-b3e01d5551f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"On May 14, 2024, the day has come: The recommendations 'Open KI for everyone!' will be presented as a festive conclusion to the Open KI in Education forum at Wikimedia Deutschland. The education scientist and pedagogue Nele Hirsch will talk to various education and digital politicians about the recommendations.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "out = use_llm(\n",
    "    task_description = \"You will be given a text. return it translated into english. reply in json format.\", \n",
    "    task_input = \"Am 14. Mai 2024 ist es soweit: Die Empfehlungen “Offene KI für alle!” werden als feierlicher Abschluss des Forums Offene KI in der Bildung bei Wikimedia Deutschland präsentiert. Dabei wird die Bildungswissenschaftlerin und Pädagogin Nele Hirsch mit verschiedenen Bildungs- und Digitalpolitiker*innen zu den Empfehlungen ins Gespräch kommen.\",\n",
    "    forced_beginning = \"{\\n    \\\"task_solution\\\": \"\n",
    ")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70867465-5a92-4ddd-8e6a-1f0f7c6dc544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"On\", \"May\", \"14\", \"2024\", \"the\", \"day\", \"has\", \"come\", \"the\", \"recommendations\", \"Open\", \"KI\", \"for\", \"everyone\", \"will\", \"be\", \"presented\", \"as\", \"a\", \"festive\", \"conclusion\", \"to\", \"the\", \"Open\", \"KI\", \"in\", \"Education\", \"forum\", \"at\", \"Wikimedia\", \"Deutschland\", \"The\", \"education\", \"scientist\", \"and\", \"pedagogue\", \"Nele\", \"Hirsch\", \"will\n"
     ]
    }
   ],
   "source": [
    "print(use_llm(\n",
    "    task_description = \"You will be given a text. return all words in the text. reply in json format.\", \n",
    "    task_input = out,\n",
    "    forced_beginning = \"{\\n    \\\"task_solution\\\": \"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf3e77c6-409f-4ddb-aa79-607619ed0bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(use_llm(\n",
    "    task_description = \"You will be given a string. decide if it is an english noun / pronoun (\\\"task_solution\\\": true) or not (\\\"task_solution\\\": false). reply in json format.\", \n",
    "    task_input = \"\\\"narr\\\"\",\n",
    "    forced_beginning = \"{\\n    \\\"task_solution\\\": \"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a562989-5da4-4ec1-97f5-bf5f358aa25a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
