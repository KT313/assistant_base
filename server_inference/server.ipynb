{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8616c622-b9f6-4e40-9067-688b3f47cad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prime_numbers = [2]\n",
      "for i in range(3, 100):\n",
      " is_prime = True\n",
      " for j in prime_numbers:\n",
      " if i % j == 0:\n",
      " is_prime = False\n",
      " break\n",
      " if is_prime:\n",
      " prime_numbers.append(i)\n",
      "\n",
      "with open('prime_numbers.txt', 'w') as f:\n",
      " for prime in prime_numbers[:15]:\n",
      " f.write(str(prime) + '\\n')\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"prime_numbers = [2]\\nfor i in range(3, 100):\\n is_prime = True\\n for j in prime_numbers:\\n if i % j == 0:\\n is_prime = False\\n break\\n if is_prime:\\n prime_numbers.append(i)\\n\\nwith open('prime_numbers.txt', 'w') as f:\\n for prime in prime_numbers[:15]:\\n f.write(str(prime) + '\\\\n')\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fdb4054-c4b7-488f-8d7a-05445691cd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# garbage_collection_threshold between 0 and 1 in % of mem\n",
    "# backend native or cudaMallocAsync\n",
    "# PYTORCH_CUDA_ALLOC_CONF=<option>:<value>,<option2>:<value2>...\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = f\"garbage_collection_threshold:0.99\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23f25fc2-f7c8-48d0-89e7-7d210cff4a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request, jsonify\n",
    "import requests\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82b1ca1d-666c-4bf3-af6b-e0eb4be019b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX\n",
    "from conversation import conv_templates, SeparatorStyle\n",
    "\n",
    "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89291a66-ab6e-430d-a718-9c1f00a744e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobi/miniconda3/envs/assba/lib/python3.11/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM, LlamaF\n",
    "import bitsandbytes, flash_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e96992ac-650f-498d-9ea7-7a40d5b42525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers.models.llama.configuration_llama.LlamaConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM.\nModel type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, GPTSanJapaneseConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, NllbMoeConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, SeamlessM4TConfig, SeamlessM4Tv2Config, SwitchTransformersConfig, T5Config, UMT5Config, XLMProphetNetConfig.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m pretrained \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m][model_name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtorch_device_map\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_flash_attention_2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/assba/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    564\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    565\u001b[0m     )\n\u001b[0;32m--> 566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers.models.llama.configuration_llama.LlamaConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM.\nModel type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, GPTSanJapaneseConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, NllbMoeConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, SeamlessM4TConfig, SeamlessM4Tv2Config, SwitchTransformersConfig, T5Config, UMT5Config, XLMProphetNetConfig."
     ]
    }
   ],
   "source": [
    "model_name = \"Hermes-2-Theta-Llama-3-8B\"\n",
    "with open(f\"../config.json\", \"r\") as config_file:\n",
    "    config = json.loads(config_file.read().strip())\n",
    "\n",
    "pretrained = config['models'][model_name]['path']\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained, trust_remote_code=False)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    pretrained,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=config['torch_device_map'],\n",
    "    load_in_8bit=False,\n",
    "    load_in_4bit=False,\n",
    "    use_flash_attention_2=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c36bbbd2-cb2a-47ae-b6cb-fc98b910f014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools: <tools> {\"type\": \"function\", \"function\": {\"name\": \"get_web_info\", \"description\": \"get_web_info(symbol: str) -> dict - Get web results for a given query.\\n\\n    Args:\\n        query (str): The web search query.\\n\\n    Returns:\\n        dict: A dictionary containing web search results.\\n            Keys:\\n                - 'website': The first returned website.\\n                - 'website_content': The content of the website.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"query\": {\"type\": \"string\"}}, \"required\": [\"query\"]}}} {\"type\": \"function\", \"function\": {\"name\": \"run_python\", \"description\": \"run_python(code: str) -> dict - Returns stdout and stderr from running the provided coda.\\n\\n    Args:\\n        code (str): The python code to run.\\n\\n    Returns:\\n        dict: A dictionary the outputs.\\n            Keys:\\n                - 'stdout': stdout from running the python code.\\n                - 'stderr': stderr from running the python code.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"code\": {\"type\": \"string\"}}, \"required\": [\"code\"]}}}  </tools> Use the following pydantic model json schema for each tool call you will make: {\"properties\": {\"arguments\": {\"title\": \"Arguments\", \"type\": \"object\"}, \"name\": {\"title\": \"Name\", \"type\": \"string\"}}, \"required\": [\"arguments\", \"name\"], \"title\": \"FunctionCall\", \"type\": \"object\"} For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
      "<tool_call>\n",
      "{\"arguments\": <args-dict>, \"name\": <function-name>}\n",
      "</tool_call><|im_end|>\n",
      "<|im_start|>user\n",
      "what is the current date?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device=\"cuda\"\n",
    "\n",
    "args = {'chat': [{'role': 'System', 'content': 'Hello, I am the system.'}, {'role': 'User', 'content': 'what is the current date?'}], 'model': 'Hermes-2-Theta-Llama-3-8B', 'manual_system_prompt': '', 'use_functions': True, 'use_image': False}\n",
    "\n",
    "new_chat = []\n",
    "new_chat.append({'role': 'system', 'content': config['models'][model_name]['functions']})\n",
    "\n",
    "for entry in args['chat']:\n",
    "    if entry['role'] == \"User\":\n",
    "        new_chat.append({'role': 'user', 'content': entry['content']})\n",
    "    if entry['role'] == \"AI\":\n",
    "        new_chat.append({'role': 'assistant', 'content': entry['content']})\n",
    "\n",
    "args['chat'] = new_chat\n",
    "\n",
    "chat_string = \"\"\n",
    "\n",
    "for entry in args['chat']:\n",
    "    chat_string += f\"<|im_start|>{entry['role']}\\n\"\n",
    "    chat_string += f\"{entry['content']}<|im_end|>\\n\"\n",
    "chat_string += f\"<|im_start|>assistant\\n\"\n",
    "\n",
    "print(chat_string, flush=True)\n",
    "\n",
    "tokens = tokenizer(chat_string, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "gen_inputs = {}\n",
    "args['tokens'] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "301d4ab6-2a1e-4fed-82b3-b0717b65b3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    HammingDiversityLogitsProcessor,\n",
    "    BeamSearchScorer,\n",
    ")\n",
    "import torch\n",
    "\n",
    "# lets run diverse beam search using 6 beams\n",
    "num_beams = 6\n",
    "# define decoder start token ids\n",
    "input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
    "input_ids = input_ids * model.config.bos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57476538-f23a-4e5d-bf9f-9414f28fab5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"/home/tobi/ai/models/text_to_text/Hermes-2-Theta-Llama-3-8B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128003,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9419167d-fb3c-40cc-8d77-9078a6c7e942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training': False, '_parameters': OrderedDict([('weight', Parameter containing:\n",
      "tensor([[ 0.0015,  0.0056, -0.0028,  ...,  0.0022, -0.0013,  0.0004],\n",
      "        [-0.0027,  0.0010, -0.0006,  ...,  0.0002, -0.0014, -0.0020],\n",
      "        [ 0.0019, -0.0134,  0.0003,  ...,  0.0022,  0.0072,  0.0040],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
      "        [ 0.0000, -0.0000, -0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000]],\n",
      "       device='cuda:0', dtype=torch.float16, requires_grad=True))]), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_pre_hooks': OrderedDict(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_hooks_with_kwargs': OrderedDict(), '_forward_hooks_always_called': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_forward_pre_hooks_with_kwargs': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_state_dict_pre_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_load_state_dict_post_hooks': OrderedDict(), '_modules': OrderedDict(), 'num_embeddings': 128256, 'embedding_dim': 4096, 'padding_idx': None, 'max_norm': None, 'norm_type': 2.0, 'scale_grad_by_freq': False, 'sparse': False, '_is_hf_initialized': True}\n"
     ]
    }
   ],
   "source": [
    "print(model.__dict__['_modules']['model'].embed_tokens.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c827ad02-423d-4d03-82a0-de313ad75d33",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LlamaForCausalLM' object has no attribute 'group_beam_search'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 25\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# instantiate logits processors\u001b[39;00m\n\u001b[1;32m     18\u001b[0m logits_processor \u001b[38;5;241m=\u001b[39m LogitsProcessorList(\n\u001b[1;32m     19\u001b[0m     [\n\u001b[1;32m     20\u001b[0m         HammingDiversityLogitsProcessor(\u001b[38;5;241m5.5\u001b[39m, num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, num_beam_groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m     21\u001b[0m         MinLengthLogitsProcessor(\u001b[38;5;241m5\u001b[39m, eos_token_id\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39meos_token_id),\n\u001b[1;32m     22\u001b[0m     ]\n\u001b[1;32m     23\u001b[0m )\n\u001b[0;32m---> 25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_beam_search\u001b[49m(\n\u001b[1;32m     26\u001b[0m     input_ids, beam_scorer, logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     29\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_ids[\u001b[38;5;241m0\u001b[39m][args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/assba/lib/python3.11/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaForCausalLM' object has no attribute 'group_beam_search'"
     ]
    }
   ],
   "source": [
    "\n",
    "# add encoder_outputs to model keyword arguments\n",
    "model_kwargs = {\n",
    "    \"encoder_outputs\": model.__dict__['_modules']['model'].embed_tokens(\n",
    "        args['tokens'].repeat_interleave(num_beams, dim=0)#, return_dict=True\n",
    "    )\n",
    "}\n",
    "\n",
    "# instantiate beam scorer\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size=1,\n",
    "    max_length=model.config.max_length,\n",
    "    num_beams=num_beams,\n",
    "    device=model.device,\n",
    "    num_beam_groups=3,\n",
    ")\n",
    "\n",
    "# instantiate logits processors\n",
    "logits_processor = LogitsProcessorList(\n",
    "    [\n",
    "        HammingDiversityLogitsProcessor(5.5, num_beams=6, num_beam_groups=3),\n",
    "        MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n",
    "    ]\n",
    ")\n",
    "\n",
    "outputs = model.group_beam_search(\n",
    "    input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs\n",
    ")\n",
    "\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "response = tokenizer.decode(generated_ids[0][args['tokens'].shape[-1]:], skip_special_tokens=True, clean_up_tokenization_space=True)\n",
    "output_shape = generated_ids[0][args['tokens'].shape[-1]:].shape\n",
    "returned_content = [response.strip()]\n",
    "print(returned_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8b3549-54ca-4f04-96b5-4732d6ca4f75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38197dbb-ff6d-49dd-8581-e97cf38bf289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_assisted_decoding', '_auto_class', '_autoset_attn_implementation', '_backward_compatibility_gradient_checkpointing', '_backward_hooks', '_backward_pre_hooks', '_beam_sample', '_beam_search', '_buffers', '_call_impl', '_check_and_enable_flash_attn_2', '_check_and_enable_sdpa', '_compiled_call_impl', '_constrained_beam_search', '_contrastive_search', '_convert_head_mask_to_5d', '_copy_lm_head_original_to_resized', '_create_repo', '_dispatch_accelerate_model', '_expand_inputs_for_generation', '_extract_past_from_model_output', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_from_config', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_candidate_generator', '_get_files_timestamps', '_get_initial_cache_position', '_get_logits_processor', '_get_logits_warper', '_get_name', '_get_no_split_modules', '_get_resized_embeddings', '_get_resized_lm_head', '_get_static_cache', '_get_stopping_criteria', '_greedy_search', '_group_beam_search', '_has_unfinished_sequences', '_hf_peft_config_loaded', '_hook_rss_memory_post_forward', '_hook_rss_memory_pre_forward', '_init_weights', '_initialize_weights', '_is_full_backward_hook', '_is_hf_initialized', '_is_quantized_training_enabled', '_keep_in_fp32_modules', '_keep_in_fp32_modules', '_keys_to_ignore_on_load_missing', '_keys_to_ignore_on_load_unexpected', '_keys_to_ignore_on_save', '_load_from_state_dict', '_load_pretrained_model', '_load_pretrained_model_low_mem', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_initialize_input_ids_for_generation', '_maybe_warn_non_full_backward_hook', '_merge_criteria_processor_list', '_modules', '_named_members', '_no_split_modules', '_non_persistent_buffers_set', '_parameters', '_prepare_attention_mask_for_generation', '_prepare_decoder_input_ids_for_generation', '_prepare_encoder_decoder_kwargs_for_generation', '_prepare_generated_length', '_prepare_generation_config', '_prepare_model_inputs', '_prepare_special_tokens', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_reorder_cache', '_replicate_for_data_parallel', '_resize_token_embeddings', '_sample', '_save_to_state_dict', '_set_default_torch_dtype', '_set_gradient_checkpointing', '_skip_keys_device_placement', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_supports_cache_class', '_supports_flash_attn_2', '_supports_sdpa', '_supports_static_cache', '_temporary_reorder_cache', '_tie_encoder_decoder_weights', '_tie_or_clone_weights', '_tied_weights_keys', '_update_model_kwargs_for_generation', '_upload_modified_files', '_validate_generated_length', '_validate_model_class', '_validate_model_kwargs', '_version', '_wrapped_call_impl', 'active_adapter', 'active_adapters', 'add_adapter', 'add_memory_hooks', 'add_model_tags', 'add_module', 'apply', 'base_model', 'base_model_prefix', 'bfloat16', 'buffers', 'call_super_init', 'can_generate', 'children', 'compile', 'compute_transition_scores', 'config', 'config_class', 'cpu', 'create_extended_attention_mask_for_decoder', 'cuda', 'dequantize', 'device', 'disable_adapters', 'disable_input_require_grads', 'double', 'dtype', 'dummy_inputs', 'dump_patches', 'enable_adapters', 'enable_input_require_grads', 'estimate_tokens', 'eval', 'extra_repr', 'float', 'floating_point_ops', 'forward', 'framework', 'from_pretrained', 'generate', 'generation_config', 'get_adapter_state_dict', 'get_buffer', 'get_decoder', 'get_extended_attention_mask', 'get_extra_state', 'get_head_mask', 'get_input_embeddings', 'get_memory_footprint', 'get_output_embeddings', 'get_parameter', 'get_position_embeddings', 'get_submodule', 'gradient_checkpointing_disable', 'gradient_checkpointing_enable', 'half', 'hf_device_map', 'init_weights', 'invert_attention_mask', 'ipu', 'is_gradient_checkpointing', 'is_parallelizable', 'lm_head', 'load_adapter', 'load_state_dict', 'main_input_name', 'model', 'model_tags', 'modules', 'name_or_path', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'num_parameters', 'parameters', 'post_init', 'prepare_inputs_for_generation', 'prune_heads', 'push_to_hub', 'register_backward_hook', 'register_buffer', 'register_for_auto_class', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'reset_memory_hooks_state', 'resize_position_embeddings', 'resize_token_embeddings', 'retrieve_modules_from_names', 'reverse_bettertransformer', 'save_pretrained', 'set_adapter', 'set_decoder', 'set_extra_state', 'set_input_embeddings', 'set_output_embeddings', 'share_memory', 'state_dict', 'supports_gradient_checkpointing', 'tie_weights', 'to', 'to_bettertransformer', 'to_empty', 'train', 'training', 'type', 'vocab_size', 'warn_if_padding_and_no_attention_mask', 'warnings_issued', 'xpu', 'zero_grad']\n"
     ]
    }
   ],
   "source": [
    "print(dir(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04852dac-edc7-4334-8280-2ce17997e575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fd09f3-3a17-45c3-8b48-f1c6ad5f2ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711665c2-712b-4d08-a7b6-c8e5fdfa31ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c184a511-1ff1-4951-8f0b-a76565e93983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2853cb92-5360-416a-baf0-6f726a0ef97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_memory = (torch.cuda.mem_get_info()[1] - torch.cuda.mem_get_info()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d31fe8a1-c92a-45ca-a6f6-c1370607a6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "434307072\n"
     ]
    }
   ],
   "source": [
    "print(initial_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d26e57c-5d90-4a32-a306-8b2e882fac8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 723 tensors from /home/tobi/ai/models/text_to_text/Meta-Llama-3-70B-Instruct-IQ2_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-70B-Instruct\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 28672\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 28\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  22:                      quantize.imatrix.file str              = /models/Meta-Llama-3-70B-Instruct-GGU...\n",
      "llama_model_loader: - kv  23:                   quantize.imatrix.dataset str              = ./training_data/groups_merged.txt\n",
      "llama_model_loader: - kv  24:             quantize.imatrix.entries_count i32              = 560\n",
      "llama_model_loader: - kv  25:              quantize.imatrix.chunks_count i32              = 88\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q4_K:   80 tensors\n",
      "llama_model_loader: - type q5_K:    1 tensors\n",
      "llama_model_loader: - type iq2_xs:  390 tensors\n",
      "llama_model_loader: - type iq3_s:   91 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 28672\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 70B\n",
      "llm_load_print_meta: model ftype      = IQ2_S - 2.5 bpw\n",
      "llm_load_print_meta: model params     = 70.55 B\n",
      "llm_load_print_meta: model size       = 20.71 GiB (2.52 BPW) \n",
      "llm_load_print_meta: general.name     = Meta-Llama-3-70B-Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.74 MiB\n",
      "llm_load_tensors: offloading 80 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 81/81 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   430.55 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size = 20773.91 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  1280.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   584.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 2566\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.entries_count': '560', 'quantize.imatrix.dataset': './training_data/groups_merged.txt', 'quantize.imatrix.chunks_count': '88', 'quantize.imatrix.file': '/models/Meta-Llama-3-70B-Instruct-GGUF/Meta-Llama-3-70B-Instruct.imatrix', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128001', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '8192', 'general.name': 'Meta-Llama-3-70B-Instruct', 'llama.embedding_length': '8192', 'llama.feed_forward_length': '28672', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '64', 'llama.block_count': '80', 'llama.attention.head_count_kv': '8', 'general.file_type': '28', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: llama-3\n"
     ]
    }
   ],
   "source": [
    "model = Llama(\n",
    "    model_path=\"/home/tobi/ai/models/text_to_text/Meta-Llama-3-70B-Instruct-IQ2_S.gguf\",\n",
    "    n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
    "    # seed=1337, # Uncomment to set a specific seed\n",
    "    n_ctx=1024, # Uncomment to increase the context window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a29f9af-06d3-4343-99c8-2eb168f3df96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model memory: 22640.00 MB 23739760640\n"
     ]
    }
   ],
   "source": [
    "model_memory = (torch.cuda.mem_get_info()[1] - torch.cuda.mem_get_info()[0]) - initial_memory\n",
    "print(f\"Model memory: {model_memory / 1024 / 1024:.2f} MB\", model_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "590dc448-5051-4495-af52-9e43a3323a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_forward_memory = (torch.cuda.mem_get_info()[1] - torch.cuda.mem_get_info()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0ee86f2-3acb-487a-aec0-cde54890464e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     614.86 ms\n",
      "llama_print_timings:      sample time =       7.55 ms /    17 runs   (    0.44 ms per token,  2251.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     351.08 ms /    13 tokens (   27.01 ms per token,    37.03 tokens per second)\n",
      "llama_print_timings:        eval time =     458.97 ms /    16 runs   (   28.69 ms per token,    34.86 tokens per second)\n",
      "llama_print_timings:       total time =     920.16 ms /    29 tokens\n"
     ]
    }
   ],
   "source": [
    "output = model(\n",
    "      \"Q: Name the planets in the solar system? A: \", # Prompt\n",
    "      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      echo=False # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15e68108-25ec-4632-935c-ae965495184e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory per token: 498.00 MB 522190848\n"
     ]
    }
   ],
   "source": [
    "after_forward_memory = (torch.cuda.mem_get_info()[1] - torch.cuda.mem_get_info()[0])\n",
    "token_memory = after_forward_memory - before_forward_memory\n",
    "print(f\"Memory per token: {token_memory / 1024 / 1024:.2f} MB\", token_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee895b03-cb0b-43c5-8d41-f4652a2ab2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-cd9b635a-65d9-4836-b070-0e9a417291e3', 'object': 'text_completion', 'created': 1716111721, 'model': '/home/tobi/ai/models/text_to_text/Meta-Llama-3-70B-Instruct-IQ2_S.gguf', 'choices': [{'text': ' Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus and Neptune', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 14, 'completion_tokens': 17, 'total_tokens': 31}}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7abd8034-2b13-444d-a374-5f84b89a26e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"num_alloc_retries\": 0,\n",
      "    \"num_ooms\": 0,\n",
      "    \"max_split_size\": -1,\n",
      "    \"allocation\": {\n",
      "        \"all\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"small_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"large_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        }\n",
      "    },\n",
      "    \"segment\": {\n",
      "        \"all\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"small_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"large_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        }\n",
      "    },\n",
      "    \"active\": {\n",
      "        \"all\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"small_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"large_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        }\n",
      "    },\n",
      "    \"inactive_split\": {\n",
      "        \"all\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"small_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"large_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        }\n",
      "    },\n",
      "    \"allocated_bytes\": {\n",
      "        \"all\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"small_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"large_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        }\n",
      "    },\n",
      "    \"reserved_bytes\": {\n",
      "        \"all\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"small_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"large_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        }\n",
      "    },\n",
      "    \"active_bytes\": {\n",
      "        \"all\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"small_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"large_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        }\n",
      "    },\n",
      "    \"inactive_split_bytes\": {\n",
      "        \"all\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"small_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"large_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        }\n",
      "    },\n",
      "    \"requested_bytes\": {\n",
      "        \"all\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"small_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"large_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        }\n",
      "    },\n",
      "    \"oversize_allocations\": {\n",
      "        \"current\": 0,\n",
      "        \"peak\": 0,\n",
      "        \"allocated\": 0,\n",
      "        \"freed\": 0\n",
      "    },\n",
      "    \"oversize_segments\": {\n",
      "        \"current\": 0,\n",
      "        \"peak\": 0,\n",
      "        \"allocated\": 0,\n",
      "        \"freed\": 0\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(torch.cuda.memory_stats_as_nested_dict(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d14dd52-4415-447c-8b68-278b4ba62129",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/get_init_text')\n",
    "def get_init_text():\n",
    "    returned_content = [\n",
    "        {'role': 'System', 'content': 'Hello, I am the system.'},\n",
    "    ]\n",
    "    return jsonify({'status': 'success', 'returned_content': returned_content})\n",
    "\n",
    "@app.route('/send_user_msg', methods=['POST'])\n",
    "def send_user_msg():\n",
    "    data = request.get_json()\n",
    "    user = data['user']\n",
    "    content = data['content']\n",
    "\n",
    "    url = f\"http://127.0.0.1:10000/infer\"\n",
    "    data_to_send = {}\n",
    "    data_to_send['content'] = content\n",
    "    data_to_send['use_image'] = False\n",
    "    response = requests.post(url, json=data_to_send)    \n",
    "    ai_reply = response.json()['returned_content']\n",
    "    \n",
    "    returned_content = [\n",
    "        {'role': 'User', 'content': content},\n",
    "        {'role': 'AI', 'content': \"\\n\".join(ai_reply)}\n",
    "    ]\n",
    "    print(f\"Received text from {user}: {content}\")\n",
    "    return jsonify({'status': 'success', 'returned_content': returned_content})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=14000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea69ba85-3356-442a-9def-5aba91b0fe6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
