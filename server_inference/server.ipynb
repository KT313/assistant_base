{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fdb4054-c4b7-488f-8d7a-05445691cd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# garbage_collection_threshold between 0 and 1 in % of mem\n",
    "# backend native or cudaMallocAsync\n",
    "# PYTORCH_CUDA_ALLOC_CONF=<option>:<value>,<option2>:<value2>...\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = f\"garbage_collection_threshold:0.99\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c6af623-71cb-4cae-a6b1-403f63ad2f93",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1137638757.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools: <tools> {\"type\": \"function\", \"function\": {\"name\": \"get_stock_fundamentals\", \"description\": \"get_stock_fundamentals(symbol: str) -> dict - Get fundamental data for a given stock symbol using yfinance API.\\\\n\\\\n    Args:\\\\n        symbol (str): The stock symbol.\\\\n\\\\n    Returns:\\\\n        dict: A dictionary containing fundamental data.\\\\n            Keys:\\\\n                - \\'symbol\\': The stock symbol.\\\\n                - \\'company_name\\': The long name of the company.\\\\n                - \\'sector\\': The sector to which the company belongs.\\\\n                - \\'industry\\': The industry to which the company belongs.\\\\n                - \\'market_cap\\': The market capitalization of the company.\\\\n                - \\'pe_ratio\\': The forward price-to-earnings ratio.\\\\n                - \\'pb_ratio\\': The price-to-book ratio.\\\\n                - \\'dividend_yield\\': The dividend yield.\\\\n                - \\'eps\\': The trailing earnings per share.\\\\n                - \\'beta\\': The beta value of the stock.\\\\n                - \\'52_week_high\\': The 52-week high price of the stock.\\\\n                - \\'52_week_low\\': The 52-week low price of the stock.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"symbol\": {\"type\": \"string\"}}, \"required\": [\"symbol\"]}}}  </tools> Use the following pydantic model json schema for each tool call you will make: {\"properties\": {\"arguments\": {\"title\": \"Arguments\", \"type\": \"object\"}, \"name\": {\"title\": \"Name\", \"type\": \"string\"}}, \"required\": [\"arguments\", \"name\"], \"title\": \"FunctionCall\", \"type\": \"object\"} For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\u001b[0m\n\u001b[0m                                                                                                                                                                                                                                                                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools: <tools> {\"type\": \"function\", \"function\": {\"name\": \"get_stock_fundamentals\", \"description\": \"get_stock_fundamentals(symbol: str) -> dict - Get fundamental data for a given stock symbol using yfinance API.\\\\n\\\\n    Args:\\\\n        symbol (str): The stock symbol.\\\\n\\\\n    Returns:\\\\n        dict: A dictionary containing fundamental data.\\\\n            Keys:\\\\n                - \\'symbol\\': The stock symbol.\\\\n                - \\'company_name\\': The long name of the company.\\\\n                - \\'sector\\': The sector to which the company belongs.\\\\n                - \\'industry\\': The industry to which the company belongs.\\\\n                - \\'market_cap\\': The market capitalization of the company.\\\\n                - \\'pe_ratio\\': The forward price-to-earnings ratio.\\\\n                - \\'pb_ratio\\': The price-to-book ratio.\\\\n                - \\'dividend_yield\\': The dividend yield.\\\\n                - \\'eps\\': The trailing earnings per share.\\\\n                - \\'beta\\': The beta value of the stock.\\\\n                - \\'52_week_high\\': The 52-week high price of the stock.\\\\n                - \\'52_week_low\\': The 52-week low price of the stock.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"symbol\": {\"type\": \"string\"}}, \"required\": [\"symbol\"]}}}  </tools> Use the following pydantic model json schema for each tool call you will make: {\"properties\": {\"arguments\": {\"title\": \"Arguments\", \"type\": \"object\"}, \"name\": {\"title\": \"Name\", \"type\": \"string\"}}, \"required\": [\"arguments\", \"name\"], \"title\": \"FunctionCall\", \"type\": \"object\"} For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
    "<tool_call>\n",
    "{\"arguments\": <args-dict>, \"name\": <function-name>}\n",
    "</tool_call>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23f25fc2-f7c8-48d0-89e7-7d210cff4a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request, jsonify\n",
    "import requests\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82b1ca1d-666c-4bf3-af6b-e0eb4be019b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX\n",
    "from conversation import conv_templates, SeparatorStyle\n",
    "\n",
    "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "import copy\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2853cb92-5360-416a-baf0-6f726a0ef97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_memory = (torch.cuda.mem_get_info()[1] - torch.cuda.mem_get_info()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d31fe8a1-c92a-45ca-a6f6-c1370607a6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "434307072\n"
     ]
    }
   ],
   "source": [
    "print(initial_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d26e57c-5d90-4a32-a306-8b2e882fac8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 723 tensors from /home/tobi/ai/models/text_to_text/Meta-Llama-3-70B-Instruct-IQ2_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-70B-Instruct\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 28672\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 28\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  22:                      quantize.imatrix.file str              = /models/Meta-Llama-3-70B-Instruct-GGU...\n",
      "llama_model_loader: - kv  23:                   quantize.imatrix.dataset str              = ./training_data/groups_merged.txt\n",
      "llama_model_loader: - kv  24:             quantize.imatrix.entries_count i32              = 560\n",
      "llama_model_loader: - kv  25:              quantize.imatrix.chunks_count i32              = 88\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q4_K:   80 tensors\n",
      "llama_model_loader: - type q5_K:    1 tensors\n",
      "llama_model_loader: - type iq2_xs:  390 tensors\n",
      "llama_model_loader: - type iq3_s:   91 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 28672\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 70B\n",
      "llm_load_print_meta: model ftype      = IQ2_S - 2.5 bpw\n",
      "llm_load_print_meta: model params     = 70.55 B\n",
      "llm_load_print_meta: model size       = 20.71 GiB (2.52 BPW) \n",
      "llm_load_print_meta: general.name     = Meta-Llama-3-70B-Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.74 MiB\n",
      "llm_load_tensors: offloading 80 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 81/81 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   430.55 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size = 20773.91 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  1280.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   584.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 2566\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.entries_count': '560', 'quantize.imatrix.dataset': './training_data/groups_merged.txt', 'quantize.imatrix.chunks_count': '88', 'quantize.imatrix.file': '/models/Meta-Llama-3-70B-Instruct-GGUF/Meta-Llama-3-70B-Instruct.imatrix', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128001', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '8192', 'general.name': 'Meta-Llama-3-70B-Instruct', 'llama.embedding_length': '8192', 'llama.feed_forward_length': '28672', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '64', 'llama.block_count': '80', 'llama.attention.head_count_kv': '8', 'general.file_type': '28', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: llama-3\n"
     ]
    }
   ],
   "source": [
    "model = Llama(\n",
    "    model_path=\"/home/tobi/ai/models/text_to_text/Meta-Llama-3-70B-Instruct-IQ2_S.gguf\",\n",
    "    n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
    "    # seed=1337, # Uncomment to set a specific seed\n",
    "    n_ctx=1024, # Uncomment to increase the context window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a29f9af-06d3-4343-99c8-2eb168f3df96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model memory: 22640.00 MB 23739760640\n"
     ]
    }
   ],
   "source": [
    "model_memory = (torch.cuda.mem_get_info()[1] - torch.cuda.mem_get_info()[0]) - initial_memory\n",
    "print(f\"Model memory: {model_memory / 1024 / 1024:.2f} MB\", model_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "590dc448-5051-4495-af52-9e43a3323a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_forward_memory = (torch.cuda.mem_get_info()[1] - torch.cuda.mem_get_info()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0ee86f2-3acb-487a-aec0-cde54890464e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     614.86 ms\n",
      "llama_print_timings:      sample time =       7.55 ms /    17 runs   (    0.44 ms per token,  2251.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     351.08 ms /    13 tokens (   27.01 ms per token,    37.03 tokens per second)\n",
      "llama_print_timings:        eval time =     458.97 ms /    16 runs   (   28.69 ms per token,    34.86 tokens per second)\n",
      "llama_print_timings:       total time =     920.16 ms /    29 tokens\n"
     ]
    }
   ],
   "source": [
    "output = model(\n",
    "      \"Q: Name the planets in the solar system? A: \", # Prompt\n",
    "      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      echo=False # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15e68108-25ec-4632-935c-ae965495184e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory per token: 498.00 MB 522190848\n"
     ]
    }
   ],
   "source": [
    "after_forward_memory = (torch.cuda.mem_get_info()[1] - torch.cuda.mem_get_info()[0])\n",
    "token_memory = after_forward_memory - before_forward_memory\n",
    "print(f\"Memory per token: {token_memory / 1024 / 1024:.2f} MB\", token_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee895b03-cb0b-43c5-8d41-f4652a2ab2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-cd9b635a-65d9-4836-b070-0e9a417291e3', 'object': 'text_completion', 'created': 1716111721, 'model': '/home/tobi/ai/models/text_to_text/Meta-Llama-3-70B-Instruct-IQ2_S.gguf', 'choices': [{'text': ' Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus and Neptune', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 14, 'completion_tokens': 17, 'total_tokens': 31}}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7abd8034-2b13-444d-a374-5f84b89a26e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"num_alloc_retries\": 0,\n",
      "    \"num_ooms\": 0,\n",
      "    \"max_split_size\": -1,\n",
      "    \"allocation\": {\n",
      "        \"all\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"small_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"large_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        }\n",
      "    },\n",
      "    \"segment\": {\n",
      "        \"all\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"small_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"large_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        }\n",
      "    },\n",
      "    \"active\": {\n",
      "        \"all\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"small_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"large_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        }\n",
      "    },\n",
      "    \"inactive_split\": {\n",
      "        \"all\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"small_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"large_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        }\n",
      "    },\n",
      "    \"allocated_bytes\": {\n",
      "        \"all\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"small_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"large_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        }\n",
      "    },\n",
      "    \"reserved_bytes\": {\n",
      "        \"all\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"small_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"large_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        }\n",
      "    },\n",
      "    \"active_bytes\": {\n",
      "        \"all\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"small_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"large_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        }\n",
      "    },\n",
      "    \"inactive_split_bytes\": {\n",
      "        \"all\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"small_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"large_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        }\n",
      "    },\n",
      "    \"requested_bytes\": {\n",
      "        \"all\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"small_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        },\n",
      "        \"large_pool\": {\n",
      "            \"current\": 0,\n",
      "            \"peak\": 0,\n",
      "            \"allocated\": 0,\n",
      "            \"freed\": 0\n",
      "        }\n",
      "    },\n",
      "    \"oversize_allocations\": {\n",
      "        \"current\": 0,\n",
      "        \"peak\": 0,\n",
      "        \"allocated\": 0,\n",
      "        \"freed\": 0\n",
      "    },\n",
      "    \"oversize_segments\": {\n",
      "        \"current\": 0,\n",
      "        \"peak\": 0,\n",
      "        \"allocated\": 0,\n",
      "        \"freed\": 0\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(torch.cuda.memory_stats_as_nested_dict(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d14dd52-4415-447c-8b68-278b4ba62129",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/get_init_text')\n",
    "def get_init_text():\n",
    "    returned_content = [\n",
    "        {'role': 'System', 'content': 'Hello, I am the system.'},\n",
    "    ]\n",
    "    return jsonify({'status': 'success', 'returned_content': returned_content})\n",
    "\n",
    "@app.route('/send_user_msg', methods=['POST'])\n",
    "def send_user_msg():\n",
    "    data = request.get_json()\n",
    "    user = data['user']\n",
    "    content = data['content']\n",
    "\n",
    "    url = f\"http://127.0.0.1:10000/infer\"\n",
    "    data_to_send = {}\n",
    "    data_to_send['content'] = content\n",
    "    data_to_send['use_image'] = False\n",
    "    response = requests.post(url, json=data_to_send)    \n",
    "    ai_reply = response.json()['returned_content']\n",
    "    \n",
    "    returned_content = [\n",
    "        {'role': 'User', 'content': content},\n",
    "        {'role': 'AI', 'content': \"\\n\".join(ai_reply)}\n",
    "    ]\n",
    "    print(f\"Received text from {user}: {content}\")\n",
    "    return jsonify({'status': 'success', 'returned_content': returned_content})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=14000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea69ba85-3356-442a-9def-5aba91b0fe6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
